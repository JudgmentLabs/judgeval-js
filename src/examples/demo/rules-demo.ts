import * as dotenv from 'dotenv';
import OpenAI from 'openai';
// No longer need base-scorer import directly if using concrete scorer
// import { APIJudgmentScorer } from '../scorers/base-scorer';
import { AnswerRelevancyScorer } from '../../scorers/api-scorer.js'; // Added .js extension
import { Tracer, wrap, TraceClient } from '../../common/tracer.js'; // Ensure Rule is NOT imported from here
import { JudgmentClient } from '../../judgment-client.js';
import { ExampleBuilder } from '../../data/example.js';
import { FaithfulnessScorer } from '../../scorers/api-scorer.js';
import { Rule, Condition, NotificationConfig } from '../../rules.js'; // Import NotificationConfig

// Load environment variables from .env file
dotenv.config({ path: '.env.local' });

// Ensure necessary environment variables are set
if (!process.env.OPENAI_API_KEY) {
  console.error("Error: OPENAI_API_KEY environment variable is not set.");
  process.exit(1);
}
if (!process.env.JUDGMENT_API_KEY) {
  console.warn("Warning: JUDGMENT_API_KEY environment variable is not set. Tracing will be disabled.");
}
if (!process.env.JUDGMENT_ORG_ID) {
  console.warn("Warning: JUDGMENT_ORG_ID environment variable is not set. Tracing will be disabled.");
}

// --- Instantiate the actual Scorer ---
// Threshold can be defined here or overridden by rule condition processing on backend
const answerRelevancyScorer = new AnswerRelevancyScorer(0.8, undefined, false, true, true, true);

// --- Define a Sample Rule using the Rule class constructor ---
const sampleRule = new Rule(
  "AnswerRelevancyThreshold", // name
  [new Condition(answerRelevancyScorer)], // conditions
  "all", // combine_type
  "Checks if the Answer Relevancy score meets a threshold.", // description
  new NotificationConfig( // notification
    true,
    ["email"],
    ["minh@judgmentlabs.ai"]
  )
  // ruleId is optional and will be generated by the constructor
);

// --- Main Demo Function ---
async function runRulesDemo() {
  // 1. Initialize the Tracer
  const tracer = Tracer.getInstance({
    projectName: 'js-rules-demo-project',
  });

  // 2. Create an OpenAI client instance
  const rawOpenai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });

  // 3. Wrap the OpenAI client
  const openai = wrap(rawOpenai);
  console.log('OpenAI client wrapped for tracing.');

  // 4. Use the wrapped client within a trace context, passing the rule
  const traceName = 'openai-rules-demo-trace';
  const userInput = 'What is the capital of Germany?';
  console.log(`\nStarting trace with rule: ${traceName}`);

  try {
    // Assign the result of runInTrace to a variable (it will be void)
    for (const trace of tracer.trace(traceName, { rules: [sampleRule] })) {
      console.log("Trace started...");

      // Run the evaluation (could be async or sync)
      // Use the evaluate method for simplicity
      for (const span of trace.span("evaluation_step", { spanType: "tool" })) {
        const openAIResponse = await openai.chat.completions.create({
          model: 'gpt-3.5-turbo',
          messages: [
            { role: 'system', content: 'You are a helpful assistant.' },
            { role: 'user', content: userInput },
          ],
          temperature: 0.7,
          max_tokens: 50,
        });

        const responseContent = openAIResponse.choices[0].message?.content ?? "";
        console.log('OpenAI API call successful.');

        // Log the response *within* the span where it's available
        console.log("\n--- OpenAI Response (within span) ---");
        if (openAIResponse && openAIResponse.choices && openAIResponse.choices.length > 0) {
          console.log('Assistant:', openAIResponse.choices[0].message?.content);
          console.log('Usage:', openAIResponse.usage);
        } else {
          console.log('No response content found.');
          console.log('Full Response:', JSON.stringify(openAIResponse, null, 2));
        }
        console.log('-----------------------------------');

        // *** ADDING asyncEvaluate call ***
        console.log('Calling asyncEvaluate...');
        await trace.asyncEvaluate(
          [answerRelevancyScorer], // <<< Use the actual scorer instance
          {
            input: userInput,
            actualOutput: "Lies", // Using hardcoded "Lies" for testing rule failure
            model: 'gpt-3.5-turbo', // <<< ADDED model name
          }
        );
        console.log('asyncEvaluate finished.');
        // **********************************
      }
    }

  } catch (error) {
    console.error("\n--- Error during demo execution ---");
    console.error(error);
    console.log('-----------------------------------');
  } finally {
    console.log(`\nTrace finished. Check Judgment dashboard for the trace and evaluation results associated with the 'answer_relevancy' scorer.`);
  }
}

// Run the demo
runRulesDemo();

// To run:
// Ensure .env.local has OPENAI_API_KEY, JUDGMENT_API_KEY, JUDGMENT_ORG_ID
// npm install
// npx ts-node src/demo/rules-demo.ts 